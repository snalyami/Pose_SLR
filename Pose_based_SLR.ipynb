{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9pRXteUaJ74"
      },
      "source": [
        "# Pose-based Sign Language Recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtnavDSn7mtb"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb6QJPef7gEk"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time , random\n",
        "import mediapipe as mp\n",
        "import copy\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense,Dropout,BatchNormalization,Input,Conv1D,MaxPooling1D,\\\n",
        "                                    TimeDistributed,Activation,Lambda,ReLU,Conv1D,ConvLSTM1D,Flatten\n",
        "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint,EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score,confusion_matrix, classification_report\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop, SGD\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import seaborn as sns\n",
        "from PIL import ImageFont, ImageDraw, Image\n",
        "import re # for preprocessing text\n",
        "from tcn import TCN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the KArSL-100 video dataset for Arabic sign language recognition from https://hamzah-luqman.github.io/KArSL/"
      ],
      "metadata": {
        "id": "Rcz36A_Az3HD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBq8LKE67gEy"
      },
      "source": [
        "# 1. Create Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb8MY9Ti7gEz",
        "outputId": "12ba5fb8-67d8-4db4-c220-0e40b7d7cb97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     SignID        Sign\n",
            "0         1           0\n",
            "1         2           1\n",
            "2         3           2\n",
            "3         4           3\n",
            "4         5           4\n",
            "..      ...         ...\n",
            "195     196  أم  mother\n",
            "196     197  أخت sister\n",
            "197     198  أخ brother\n",
            "198     199   بنت  girl\n",
            "199     200   رضيع baby\n",
            "\n",
            "[200 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Read labels\n",
        "import pandas as pd\n",
        "labels = pd.read_csv('./KARSL_Labels2.csv')\n",
        "print (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpROSd_n7gEz",
        "outputId": "5f3e9627-3ff2-46c8-8ce1-8ad6da71ec2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               0\n",
              "1               1\n",
              "2               2\n",
              "3               3\n",
              "4               4\n",
              "          ...    \n",
              "195    أم  mother\n",
              "196    أخت sister\n",
              "197    أخ brother\n",
              "198     بنت  girl\n",
              "199     رضيع baby\n",
              "Name: Sign, Length: 200, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "labels['Sign']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay3kzXOJ7gE0",
        "outputId": "c41d05aa-b636-4fb0-b305-85437be865c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "# Use only 100 classes\n",
        "actions = np.array(labels[70:170])\n",
        "print(len(actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2d76ch-7gE0",
        "outputId": "29936a4e-c476-4de7-c308-23ddb3a13e7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([71, 'هيكل عظمي'], dtype=object)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyEAnxH_7gE0",
        "outputId": "5c99313f-ebbe-4eaf-fbea-252df102bd05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([170, 'يقفل ( يغلق ) close'], dtype=object)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actions[99]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MQp-mbj7gEq"
      },
      "source": [
        "# 2. Extract Keypoints using MP Holistic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MEsTpCH7gEw"
      },
      "source": [
        "## Setup Folders for Keypoint Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YFhFhvn7gEx"
      },
      "outputs": [],
      "source": [
        "# Create folders to store the extracted keypoints\n",
        "\n",
        "# 50 videos per sign per signer\n",
        "no_sequences = 50\n",
        "\n",
        "# Videos are going to be 30 frames in length\n",
        "sequence_length = 30\n",
        "\n",
        "DATA_PATH = \"./MP_Data\"\n",
        "\n",
        "# Create a folder for each signer, then folder for each action, then folder for each video\n",
        "for action in range(1, number_of_actions+1):\n",
        "    for sequence in range(1,no_sequences+1):\n",
        "        try:\n",
        "            # name of action folder is the code of the action e.g. 0001 for action 1\n",
        "            os.makedirs(os.path.join(DATA_PATH,\"signer1\",str(action).zfill(4), str(sequence)))\n",
        "        except:\n",
        "            pass\n",
        "for action in range(1, number_of_actions+1):\n",
        "    for sequence in range(1,no_sequences+1):\n",
        "        try:\n",
        "            # name of action folder is the code of the action e.g. 0001 for action 1\n",
        "            os.makedirs(os.path.join(DATA_PATH,\"signer2\",str(action).zfill(4), str(sequence)))\n",
        "        except:\n",
        "            pass\n",
        "for action in range(1, number_of_actions+1):\n",
        "    for sequence in range(1,no_sequences+1):\n",
        "        try:\n",
        "            # name of action folder is the code of the action e.g. 0001 for action 1\n",
        "            os.makedirs(os.path.join(DATA_PATH,\"signer3\",str(action).zfill(4), str(sequence)))\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbkCpq3n7gEx"
      },
      "source": [
        "## Extract joint landmarks from video dataset (33 pose landmarks, 468 face landmarks, and 21 hand landmarks per hand).\n",
        "Each landmark is represented by three points x, y and z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTV-oXmg7gEx"
      },
      "outputs": [],
      "source": [
        "signers = [\"01\",  \"02\", \"03\"]\n",
        "for signer in signers:\n",
        "    Videos_Data_path = f\"/Volumes/kfupm/KArSL/{signer}\"\n",
        "    countVideosPerAction = [0] * len(actions)\n",
        "\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        for root, dirs, files in os.walk(Videos_Data_path):\n",
        "            for file in files:\n",
        "                filename, extension = os.path.splitext(file)\n",
        "                if extension == '.mp4' and not filename.startswith('.'):\n",
        "                    cap = cv2.VideoCapture(os.path.join(root, file))\n",
        "                    filename2 = filename.split(\"_\")\n",
        "\n",
        "                    try:\n",
        "                        actionCode = int(filename2[2])\n",
        "                    except (IndexError, ValueError):\n",
        "                        print(f\"Skipping file due to filename format: {file}\")\n",
        "                        continue\n",
        "\n",
        "                    if actionCode < 1 or actionCode > len(actions):\n",
        "                        print(f\"Invalid action code: {actionCode} in file {file}\")\n",
        "                        continue\n",
        "\n",
        "                    countVideosPerAction[actionCode - 1] += 1\n",
        "                    if countVideosPerAction[actionCode - 1] > 50:\n",
        "                        cap.release()\n",
        "                        continue\n",
        "\n",
        "                    for frame_num in range(1, sequence_length + 1):\n",
        "                        ret, frame = cap.read()\n",
        "                        if ret:\n",
        "                            image, results = mediapipe_detection(frame, holistic)\n",
        "                            draw_styled_landmarks(image, results)\n",
        "                            keypoints = extract_keypoints(results)\n",
        "                        else:\n",
        "                            print(f\"Missing frame in {file}, frame #{frame_num}\")\n",
        "                            # Repeat last keypoints if available\n",
        "                            if frame_num == 1:\n",
        "                                keypoints = np.zeros(1662,)  # or appropriate default\n",
        "                        # Save keypoints\n",
        "                        save_dir = os.path.join(DATA_PATH, f\"signer{signer}\", str(actionCode), str(countVideosPerAction[actionCode - 1]))\n",
        "                        os.makedirs(save_dir, exist_ok=True)\n",
        "                        npy_path = os.path.join(save_dir, f\"{frame_num}.npy\")\n",
        "                        np.save(npy_path, keypoints)\n",
        "\n",
        "                        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "                            break\n",
        "\n",
        "                    cap.release()\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TqCRgPP7gE0"
      },
      "source": [
        "## Load data for signer 1, 2 and 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfd3lhjl7gE0"
      },
      "outputs": [],
      "source": [
        "def load_features(signer, include_nonManual= False):\n",
        "    # Load extracted keypoints from saved npy files\n",
        "    DATA_PATH_KEYPOINTS = os.path.join('./MP_Data/',signer)\n",
        "    sequences, labels = [], []\n",
        "    for action in range(70,170):\n",
        "        for sequence in range (1, 51):\n",
        "            window = []\n",
        "            for frame_num in range(1,31):\n",
        "                #Load Frame Keypoints\n",
        "                res1 = np.load(os.path.join(DATA_PATH_KEYPOINTS, str(action).zfill(4), str(sequence), \"{}.npy\".format(frame_num)))\n",
        "                #Take Hands Landmarks\n",
        "                lh_rh = res1[1536:] # extract hand landmarks from npy file\n",
        "                pose = res1[0:132]   # extract pose landmarks from npy file\n",
        "\n",
        "                #Remove z Axis From Landmarks\n",
        "                for z in range(2,lh_rh.shape[0],3):\n",
        "                        lh_rh[z] = None\n",
        "                for z in range(2,pose.shape[0],3):\n",
        "                        pose[z] = None\n",
        "                #Romove visibilty indicator from pose\n",
        "                for z in range(2,pose.shape[0],4):\n",
        "                        pose[z] = None\n",
        "\n",
        "                #Remove NaN Data\n",
        "                lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n",
        "                pose = pose[np.logical_not(np.isnan(pose))]\n",
        "\n",
        "                if (include_nonManual):\n",
        "                    features = np.concatenate([pose,lh_rh]) # Concatenate manual/non-manual features\n",
        "                else:\n",
        "                    features = lh_rh\n",
        "                #print(len(pose))\n",
        "                #print(len(lh_rh))\n",
        "                #print(len(features))\n",
        "                window.append(features)\n",
        "            sequences.append(window)\n",
        "            labels.append(action-70)\n",
        "    return sequences, labels\n",
        "\n",
        "# Load extracted features for each signer\n",
        "sequences1, labels1 = load_features(\"signer1\", True)\n",
        "sequences2, labels2 = load_features(\"signer2\", True)\n",
        "sequences3, labels3 = load_features(\"signer3\", True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9zdOgm27gE1",
        "outputId": "4321e594-c076-4d3a-c930-f694f73c7f02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 30, 150)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(sequences3).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZhdtSv07gE1",
        "outputId": "e46e55c8-f68b-4118-c5ec-234fd53f4f21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((5000, 30, 150), (5000, 100))"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Convert Lists To Array\n",
        "X1 = np.array(sequences1)\n",
        "#Convert Labels to OHE\n",
        "y1 = to_categorical(labels1).astype(int)\n",
        "X1.shape, y1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI1LgdGh7gE2",
        "outputId": "05cbfe4b-2da3-40bc-c753-ef1fe73e7708"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((5000, 30, 150), (5000, 100))"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Convert Lists To Array\n",
        "X2 = np.array(sequences2)\n",
        "#Convert Labels to OHE\n",
        "y2 = to_categorical(labels2).astype(int)\n",
        "X2.shape, y2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl2wAV687gE2",
        "outputId": "08de4552-8663-46c8-b88d-9b52648867f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((5000, 30, 150), (5000, 100))"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Convert Lists To Array\n",
        "X3 = np.array(sequences3)\n",
        "#Convert Labels to OHE\n",
        "y3 = to_categorical(labels3).astype(int)\n",
        "X3.shape, y3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1P1BZnXiWzX"
      },
      "outputs": [],
      "source": [
        "# You can save the data in one npy file, instead of reading from the folders everytime.\n",
        "np.save(\"X1_p.npy\", X1)\n",
        "np.save(\"X2_p.npy\", X2)\n",
        "np.save(\"X3_p.npy\", X3)\n",
        "np.save(\"y1_p.npy\", y1)\n",
        "np.save(\"y2_p.npy\", y2)\n",
        "np.save(\"y3_p.npy\", y3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTOnEqnaiWzX"
      },
      "outputs": [],
      "source": [
        "X1 = np.load(\"X1.npy\")\n",
        "X2 = np.load(\"X2.npy\")\n",
        "X3 = np.load(\"X3.npy\")\n",
        "y1 = np.load(\"y1.npy\")\n",
        "y2 = np.load(\"y2.npy\")\n",
        "y3 = np.load(\"y3.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iix5hCrR7gE2"
      },
      "source": [
        "## Split data for signer-independant training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaZmoqIZ7gE2"
      },
      "outputs": [],
      "source": [
        "# e.g. for the case of training on data from signer 1,2 and testing on signer 3\n",
        "X = np.concatenate([X2,X3])\n",
        "y = np.concatenate([y2,y3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVVMtl1o7gE2"
      },
      "outputs": [],
      "source": [
        "# Split Data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20,shuffle=True,stratify=y,random_state=42)\n",
        "X_test = X1\n",
        "y_test = y1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X1\n",
        "y_test = y1"
      ],
      "metadata": {
        "id": "MmJAZ2S_r2cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkjxGWNP7gE2"
      },
      "source": [
        "# 3. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKR8pxhd7gE2"
      },
      "outputs": [],
      "source": [
        "# Rotation Augmentation\n",
        "def augment_data_rotataion(X,y):\n",
        "    '''\n",
        "    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n",
        "    output: Augmented X,y as numpy array Shape:[Samples,Timesteps,Features]\n",
        "    '''\n",
        "    # Make an Array with Shape Like Original One\n",
        "    augmented_X = np.zeros_like(X)\n",
        "    augmented_y = np.zeros_like(y)\n",
        "\n",
        "    #Looping in all Examples\n",
        "    for ex in range(X.shape[0]):\n",
        "        # Get Random Angle Betwwen -5,5\n",
        "        rotation_angle = random.randint(-5,5)\n",
        "        # Convert it to Radians\n",
        "        theta = np.radians(rotation_angle)\n",
        "        c, s = np.cos(theta), np.sin(theta)\n",
        "        # Build a Rotation Matrix\n",
        "        rotation_matrix = np.array(((c, -s), (s, c)))\n",
        "        # Looping Each Frame\n",
        "        for frame in range(X.shape[1]):\n",
        "            window = []\n",
        "            # looping each Point within Frame\n",
        "            for i in range(0,X.shape[2]-1,2):\n",
        "                # Get Keypoint\n",
        "                keypoint = np.array([X[ex][frame][i],X[ex][frame][i+1]])\n",
        "                # Calculate Rotated Keypoint\n",
        "                rotated_keypoint = np.dot(rotation_matrix, keypoint)\n",
        "                keypoint_x = rotated_keypoint[0]\n",
        "                keypoint_y = rotated_keypoint[1]\n",
        "                # Append New Keypoint To our Data\n",
        "                window.extend([keypoint_x,keypoint_y])\n",
        "            augmented_X[ex][frame] = np.array(window)\n",
        "        augmented_y[ex] = y[ex]\n",
        "    return augmented_X,augmented_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjj9GRTM7gE2"
      },
      "outputs": [],
      "source": [
        "# Scale Augmentation\n",
        "def augment_data_scale(X,y):\n",
        "    '''\n",
        "    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n",
        "    output: Augmented X,y as numpy array Shape: [Samples,Timesteps,Features]\n",
        "    '''\n",
        "    # Make an Array with Shape Like Original One\n",
        "    augmented_X = np.zeros_like(X)\n",
        "    augmented_y = np.zeros_like(y)\n",
        "    # Looping in Each Sample\n",
        "    for ex in range(X.shape[0]):\n",
        "        # Get Random Scale Factor\n",
        "        SCALE = round(random.random(),2)\n",
        "        for frame in range(X.shape[1]):\n",
        "            # Calculate New Point\n",
        "            augmented_X[ex][frame] = X[ex][frame]*SCALE\n",
        "        augmented_y[ex] = y[ex]\n",
        "    return augmented_X,augmented_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sTs0zOk7gE3"
      },
      "outputs": [],
      "source": [
        "# Augmented Rotated Data\n",
        "rot_x,rot_y = augment_data_rotataion(X_train,y_train)\n",
        "# Augmented Scaled Data\n",
        "scaled_x,scaled_y = augment_data_scale(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzTuiwXn7gE3",
        "outputId": "38f73bfe-4a6d-4074-f52c-3329feabad2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 30, 84), (24000, 100))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# Concatenate all data [Original and Augmented]\n",
        "X_train = np.concatenate([X_train,rot_x,scaled_x])\n",
        "y_train = np.concatenate([y_train,rot_y,scaled_y])\n",
        "X_train.shape,y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6EbEN5e7gE3",
        "outputId": "7ec35ccf-94a1-4885-8902-28f0d57faa54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 30, 84),\n",
              " (5000, 30, 84),\n",
              " (2000, 30, 84),\n",
              " (2000, 100),\n",
              " (24000, 100),\n",
              " (5000, 100))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "X_train.shape, X_test.shape,X_val.shape, y_val.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz1eGOpE7gE3"
      },
      "source": [
        "# 4. Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogBywQUO7gE3"
      },
      "outputs": [],
      "source": [
        "# Define time steps\n",
        "timesteps = X.shape[1]\n",
        "\n",
        "# Define number of features\n",
        "features = X.shape[2]\n",
        "\n",
        "def model_predict(model, X_pred, y_pred):\n",
        "    yhat = model.predict(X_pred)\n",
        "    ytrue = np.argmax(y_pred, axis=1).tolist()\n",
        "    yhat = np.argmax(yhat, axis=1).tolist()\n",
        "    print(accuracy_score(ytrue, yhat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkaOwAa07gE4"
      },
      "source": [
        "## 4.1 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I-YXLSb7gE4",
        "outputId": "be75b6a7-037b-4982-a4b3-9c0c3c33a01e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 30, 64)            55040     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               3300      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 212,804\n",
            "Trainable params: 212,804\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build a LSTM Model Arch\n",
        "model_lstm3 = Sequential()\n",
        "model_lstm3.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(timesteps,features))) # frames * Features\n",
        "model_lstm3.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model_lstm3.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model_lstm3.add(Dense(64, activation='relu'))\n",
        "model_lstm3.add(Dense(32, activation='relu'))\n",
        "model_lstm3.add(Dense(actions.shape[0], activation='softmax'))\n",
        "model_lstm3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmqB0RWY7gE4"
      },
      "outputs": [],
      "source": [
        "# Compilation Configuration\n",
        "model_lstm3.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Br_PSBt7gE4"
      },
      "outputs": [],
      "source": [
        "# Define Callbacks\n",
        "log_dir = os.path.join('Logs/LSTM_test_on_2_p')\n",
        "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
        "                          update_freq='epoch',\n",
        "                          profile_batch=0) ## !tensorboard --logdir=.\n",
        "mc = ModelCheckpoint('Models/LSTM_test_on_2_p.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=10)\n",
        "callbacks = [tb_callback,mc,es]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFeMxKxiWzd"
      },
      "outputs": [],
      "source": [
        "model_lstm3.fit(X_train, y_train, epochs=1000, callbacks=[callbacks],batch_size=32,validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFrX9Eel7gE5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxj1e7ou7gE5"
      },
      "outputs": [],
      "source": [
        "# load best model, saved by early stopping\n",
        "loaded_model = tf.keras.models.load_model('Models/LSTM_test_on_2_p.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb6Segsy7gE5"
      },
      "outputs": [],
      "source": [
        "model_predict(loaded_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLulNk7T7gE6"
      },
      "source": [
        "## 4.2 Temporal Convolutional Network (TCN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTz0veu27gE6"
      },
      "outputs": [],
      "source": [
        "# from https://github.com/philipperemy/keras-tcn\n",
        "!pip install keras-tcn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ_QHPhE7gE6",
        "outputId": "f7aed92f-f3c1-4529-bba1-af698eed021d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tcn (TCN)                   (None, 30, 64)            174400    \n",
            "                                                                 \n",
            " tcn_1 (TCN)                 (None, 30, 128)           575104    \n",
            "                                                                 \n",
            " tcn_2 (TCN)                 (None, 64)                168768    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 100)               3300      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 927,812\n",
            "Trainable params: 927,812\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build a TCN Model Arch\n",
        "model_TCN3 = Sequential()\n",
        "model_TCN3.add(TCN(64, return_sequences=True, activation='relu', input_shape=(timesteps,features))) # frames * Features\n",
        "model_TCN3.add(TCN(128, return_sequences=True, activation='relu'))\n",
        "model_TCN3.add(TCN(64))\n",
        "model_TCN3.add(Dense(64, activation='relu'))\n",
        "model_TCN3.add(Dense(32, activation='relu'))\n",
        "model_TCN3.add(Dense(actions.shape[0], activation='softmax'))\n",
        "\n",
        "model_TCN3.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "model_TCN3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBYksegR7gE7"
      },
      "outputs": [],
      "source": [
        "# Define Callbacks\n",
        "log_dir = os.path.join('/Logs/TCN_test_on_1_p')\n",
        "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
        "                          update_freq='epoch',\n",
        "                          profile_batch=0) ## !tensorboard --logdir=.\n",
        "mc = ModelCheckpoint('/Models/TCN_test_on_1_p.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_loss', verbose=1,patience=10)\n",
        "callbacks = [tb_callback,mc,es]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RARsu7qa7gE7"
      },
      "outputs": [],
      "source": [
        "model_TCN3.fit(X_train, y_train, epochs=1000, callbacks=[callbacks],batch_size=32,validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fYNYlKDiWzj"
      },
      "outputs": [],
      "source": [
        "loaded_model = tf.keras.models.load_model('./Models/TCN_test_on_1_11d.h5',custom_objects={'TCN': TCN})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y10S2rPziWzk"
      },
      "outputs": [],
      "source": [
        "model_predict(loaded_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsAasljAiWzk",
        "outputId": "1567162e-255a-4eb1-b103-7028bcf08549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tcn_6 (TCN)                 (None, 30, 64)            174400    \n",
            "                                                                 \n",
            " tcn_7 (TCN)                 (None, 30, 128)           575104    \n",
            "                                                                 \n",
            " tcn_8 (TCN)                 (None, 64)                168768    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               3300      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 927,812\n",
            "Trainable params: 927,812\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCb0sv3E7gE8"
      },
      "source": [
        "## 4.3 Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vl7e-FT7gE8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHRIeomd7gE8"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder from: https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
        "# Create transformwe encoder\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'sequence_length': self.sequence_length,\n",
        "            'output_dim': self.output_dim,\n",
        "\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zYMfvkriWzm"
      },
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = PositionalEmbedding(\n",
        "        input_shape[0], input_shape[1], name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(actions.shape[0], activation=\"softmax\")(x)\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6RDcnBJiWzo",
        "outputId": "b9c4c692-34e5-494c-d2df-e12a9f586698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 30, 84)]     0           []                               \n",
            "                                                                                                  \n",
            " frame_position_embedding (Posi  (None, 30, 84)      2520        ['input_4[0][0]']                \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 30, 84)      168         ['frame_position_embedding[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 30, 84)      781140      ['layer_normalization_16[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 30, 84)       0           ['multi_head_attention_8[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (TFOpL  (None, 30, 84)      0           ['dropout_22[0][0]',             \n",
            " ambda)                                                           'frame_position_embedding[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 30, 84)      168         ['tf.__operators__.add_16[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 30, 2048)     174080      ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 30, 2048)     0           ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 30, 84)       172116      ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_17 (TFOpL  (None, 30, 84)      0           ['conv1d_17[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 30, 84)      168         ['tf.__operators__.add_17[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 30, 84)      781140      ['layer_normalization_18[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 30, 84)       0           ['multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 30, 84)      0           ['dropout_24[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 30, 84)      168         ['tf.__operators__.add_18[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 30, 2048)     174080      ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 30, 2048)     0           ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 30, 84)       172116      ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 30, 84)      0           ['conv1d_19[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 30, 84)      168         ['tf.__operators__.add_19[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 30, 84)      781140      ['layer_normalization_20[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 30, 84)       0           ['multi_head_attention_10[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_20 (TFOpL  (None, 30, 84)      0           ['dropout_26[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 30, 84)      168         ['tf.__operators__.add_20[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 30, 2048)     174080      ['layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 30, 2048)     0           ['conv1d_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 30, 84)       172116      ['dropout_27[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_21 (TFOpL  (None, 30, 84)      0           ['conv1d_21[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 30, 84)      168         ['tf.__operators__.add_21[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 30, 84)      781140      ['layer_normalization_22[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 30, 84)       0           ['multi_head_attention_11[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 30, 84)      0           ['dropout_28[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 30, 84)      168         ['tf.__operators__.add_22[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 30, 2048)     174080      ['layer_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 30, 2048)     0           ['conv1d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 30, 84)       172116      ['dropout_29[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_23 (TFOpL  (None, 30, 84)      0           ['conv1d_23[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 30)          0           ['tf.__operators__.add_23[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 128)          3968        ['global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 128)          0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100)          12900       ['dropout_30[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,530,076\n",
            "Trainable params: 4,530,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "model_transformer1 = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=9,\n",
        "    ff_dim=2048,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "model_transformer1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9ZaTHXh7gE8"
      },
      "outputs": [],
      "source": [
        "model_transformer1.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    metrics=[\"categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdYQjG3x7gE9"
      },
      "outputs": [],
      "source": [
        "log_dir = os.path.join('./Logs/transformer_encoder_test_on_1_positional')\n",
        "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
        "                          update_freq='epoch',\n",
        "                          profile_batch=0) ## !tensorboard --logdir=.\n",
        "mc = ModelCheckpoint('./Models/transformer_encoder_test_on_1_positional.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=5)\n",
        "callbacks = [tb_callback,mc,es]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_transformer1.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val,y_val),\n",
        "    epochs=1000,\n",
        "    batch_size=64, #8\n",
        "    #initial_epoch = 124,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "id": "vxDnJswRH2lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the saved model`\n",
        "loaded_model = tf.keras.models.load_model(\"./Models/transformer_encoder_test_on_2_positional.h5\",custom_objects={'PositionalEmbedding': PositionalEmbedding})"
      ],
      "metadata": {
        "id": "ZiMv_7lFTY8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_predict(loaded_model, X_test, y_test)"
      ],
      "metadata": {
        "id": "RgGPoOauGqyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WP0GNcz57gE6"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "aee184bebc06f3122aeb5a2069a23f71eb4630874e570a919845476df54908f4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}